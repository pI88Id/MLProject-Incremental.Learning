{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from torch.backends import cudnn\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets.vision import StandardTransform\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "from Cifar100.Cifar100 import Cifar100"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Set Arguments**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' # 'cuda' or 'cpu'\n",
    "\n",
    "NUM_CLASSES = 100\n",
    "CLASSES_EACH_TRAIN = 10\n",
    "\n",
    "BATCH_SIZE = 32     # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing\n",
    "                    # the batch size, learning rate should change by the same factor to have comparable results\n",
    "\n",
    "LR = 1e-3            # The initial Learning Rate\n",
    "MOMENTUM = 0.9       # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
    "WEIGHT_DECAY = 5e-5  # Regularization, you can keep this at the default\n",
    "\n",
    "NUM_EPOCHS = 25  #30    # Total number of training epochs (iterations over dataset)\n",
    "STEP_SIZE = 12  #30    # How many epochs before decreasing learning rate (if using a step-down policy)\n",
    "GAMMA = 0.1          # Multiplicative factor for learning rate step-down\n",
    "\n",
    "LOG_FREQUENCY = 100\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Prepare Network**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "net = resnet18()\n",
    "best_net = resnet18()\n",
    "\n",
    "# We just changed the last layer of AlexNet with a new fully connected layer with NUM_CLASSES outputs\n",
    "net.fc = nn.Linear(net.fc.in_features, NUM_CLASSES)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Prepare Training**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss() # for classification, we use Cross Entropy\n",
    "\n",
    "parameters_to_optimize = net.parameters()\n",
    "\n",
    "optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "# optimizer = optim.Adam(parameters_to_optimize, lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# Define scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Train and Test**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "net = net.to(DEVICE)\n",
    "cudnn.benchmark = True # Calling this optimizes runtime\n",
    "\n",
    "current_step = 0\n",
    "best_accuracy = 0\n",
    "loss = 0\n",
    "index = 0\n",
    "loss_train = []\n",
    "accuracy_train = []\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#New variable from our class Cifar100\n",
    "cifar100 = Cifar100(BATCH_SIZE, NUM_EPOCHS, DEVICE, LR, STEP_SIZE, GAMMA)\n",
    "\n",
    "for index in range(0, int(NUM_CLASSES/CLASSES_EACH_TRAIN)):\n",
    "\n",
    "  #Load data from Cifar100\n",
    "  train_dataloader = cifar100.load('train')#, index)\n",
    "  test_dataloader = cifar100.load('test')#, index)\n",
    "\n",
    "  # Start iterating over the epochs\n",
    "  for epoch in range(NUM_EPOCHS):\n",
    "\n",
    "    print('Starting epoch {}/{}, LR = {}'.format(epoch+1, NUM_EPOCHS, scheduler.get_lr()))\n",
    "    running_correct_train=0\n",
    "\n",
    "    # Iterate over the dataset\n",
    "    for images, labels in train_dataloader:\n",
    "\n",
    "      # Bring data over the device of choice\n",
    "      images = images.to(DEVICE)\n",
    "      labels = labels.to(DEVICE)\n",
    "\n",
    "      net.train().to(DEVICE)\n",
    "      optimizer.zero_grad()\n",
    "      outputs = net(images)\n",
    "\n",
    "      #Calc the correct for the graph\n",
    "      _, preds = torch.max(outputs.data, 1)\n",
    "      running_correct_train += torch.sum(preds == labels.data).data.item()\n",
    "\n",
    "      # Compute loss based on output and ground truth\n",
    "      loss = criterion(outputs, labels)\n",
    "\n",
    "      # Log loss\n",
    "      if current_step % LOG_FREQUENCY == 0:\n",
    "        print('Step {}, Loss {}'.format(current_step, loss.item()))\n",
    "\n",
    "      # Compute gradients for each layer and update weights\n",
    "      loss.backward()  # backward pass: computes gradients\n",
    "      optimizer.step() # update weights based on accumulated gradients\n",
    "\n",
    "      current_step += 1\n",
    "\n",
    "    loss_train.append(loss.item())\n",
    "    accuracy_train.append(running_correct_train / float(len(train_dataloader)))\n",
    "\n",
    "    # Step the scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "  accuracy_test, loss_test = cifar100.test(net, test_dataloader, DEVICE, criterion)\n",
    "\n",
    "  loss_test.append(loss.item())\n",
    "  accuracy_test.append(running_correct_train / float(len(train_dataloader)))\n",
    "  print('Test Accuracy: {}'.format(accuracy_test))\n",
    "\n",
    "\n",
    "  if accuracy_test > best_accuracy:\n",
    "    best_net = copy.deepcopy(net)\n",
    "    best_accuracy = accuracy_test\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Plots**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cifar100.plot()\n",
    "print('Best accuracy', best_accuracy)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-ca37dc",
   "language": "python",
   "display_name": "PyCharm (HW2)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}